{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "217d5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow as tf\n",
    "#from transformers import BertTokenizer, TFBertModel\n",
    "#from tensorflow.keras.layers import Input, Dense\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c14628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ethos/ethos_data/Ethos_Dataset_Multi_Label.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb557e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>violence</th>\n",
       "      <th>directed_vs_generalized</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>national_origin</th>\n",
       "      <th>disability</th>\n",
       "      <th>religion</th>\n",
       "      <th>sexual_orientation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You should know women's sports are a joke</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You look like Sloth with deeper Down’s syndrome</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You look like Russian and speak like Indian. B...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women deserve to be abused, I guess.</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women are made for making babies and cooking d...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>All people are fucked up because of these quee...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Actually THIS WEIRD PEOPLE are HARMFUL! They n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>a homo? so you fuck your brother instead of yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>(((They))) will be chased out of every white c...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  violence  \\\n",
       "0            You should know women's sports are a joke  0.000000   \n",
       "1      You look like Sloth with deeper Down’s syndrome  0.000000   \n",
       "2    You look like Russian and speak like Indian. B...  1.000000   \n",
       "3                 Women deserve to be abused, I guess.  0.666667   \n",
       "4    Women are made for making babies and cooking d...  0.000000   \n",
       "..                                                 ...       ...   \n",
       "428  All people are fucked up because of these quee...  0.000000   \n",
       "429  Actually THIS WEIRD PEOPLE are HARMFUL! They n...  0.000000   \n",
       "430  ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab...  0.000000   \n",
       "431  a homo? so you fuck your brother instead of yo...  0.000000   \n",
       "432  (((They))) will be chased out of every white c...  0.333333   \n",
       "\n",
       "     directed_vs_generalized    gender      race  national_origin  disability  \\\n",
       "0                   0.000000  1.000000  0.000000         0.000000    0.000000   \n",
       "1                   0.833333  0.000000  0.000000         0.000000    1.000000   \n",
       "2                   0.500000  0.142857  0.142857         0.714286    0.000000   \n",
       "3                   0.166667  1.000000  0.000000         0.000000    0.000000   \n",
       "4                   0.000000  1.000000  0.000000         0.000000    0.000000   \n",
       "..                       ...       ...       ...              ...         ...   \n",
       "428                 0.000000  0.571429  0.000000         0.000000    0.000000   \n",
       "429                 1.000000  0.000000  0.000000         0.333333    0.666667   \n",
       "430                 0.000000  0.000000  0.285714         0.000000    0.000000   \n",
       "431                 0.666667  0.000000  0.000000         0.000000    0.000000   \n",
       "432                 0.000000  0.000000  0.666667         0.000000    0.000000   \n",
       "\n",
       "     religion  sexual_orientation  \n",
       "0    0.000000            0.000000  \n",
       "1    0.000000            0.000000  \n",
       "2    0.000000            0.000000  \n",
       "3    0.000000            0.000000  \n",
       "4    0.000000            0.000000  \n",
       "..        ...                 ...  \n",
       "428  0.000000            0.571429  \n",
       "429  0.000000            0.000000  \n",
       "430  0.857143            0.000000  \n",
       "431  0.000000            1.000000  \n",
       "432  0.333333            0.000000  \n",
       "\n",
       "[433 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22509cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.duplicated of                                                comment  violence  \\\n",
       "0            You should know women's sports are a joke  0.000000   \n",
       "1      You look like Sloth with deeper Down’s syndrome  0.000000   \n",
       "2    You look like Russian and speak like Indian. B...  1.000000   \n",
       "3                 Women deserve to be abused, I guess.  0.666667   \n",
       "4    Women are made for making babies and cooking d...  0.000000   \n",
       "..                                                 ...       ...   \n",
       "428  All people are fucked up because of these quee...  0.000000   \n",
       "429  Actually THIS WEIRD PEOPLE are HARMFUL! They n...  0.000000   \n",
       "430  ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab...  0.000000   \n",
       "431  a homo? so you fuck your brother instead of yo...  0.000000   \n",
       "432  (((They))) will be chased out of every white c...  0.333333   \n",
       "\n",
       "     directed_vs_generalized    gender      race  national_origin  disability  \\\n",
       "0                   0.000000  1.000000  0.000000         0.000000    0.000000   \n",
       "1                   0.833333  0.000000  0.000000         0.000000    1.000000   \n",
       "2                   0.500000  0.142857  0.142857         0.714286    0.000000   \n",
       "3                   0.166667  1.000000  0.000000         0.000000    0.000000   \n",
       "4                   0.000000  1.000000  0.000000         0.000000    0.000000   \n",
       "..                       ...       ...       ...              ...         ...   \n",
       "428                 0.000000  0.571429  0.000000         0.000000    0.000000   \n",
       "429                 1.000000  0.000000  0.000000         0.333333    0.666667   \n",
       "430                 0.000000  0.000000  0.285714         0.000000    0.000000   \n",
       "431                 0.666667  0.000000  0.000000         0.000000    0.000000   \n",
       "432                 0.000000  0.000000  0.666667         0.000000    0.000000   \n",
       "\n",
       "     religion  sexual_orientation  \n",
       "0    0.000000            0.000000  \n",
       "1    0.000000            0.000000  \n",
       "2    0.000000            0.000000  \n",
       "3    0.000000            0.000000  \n",
       "4    0.000000            0.000000  \n",
       "..        ...                 ...  \n",
       "428  0.000000            0.571429  \n",
       "429  0.000000            0.000000  \n",
       "430  0.857143            0.000000  \n",
       "431  0.000000            1.000000  \n",
       "432  0.333333            0.000000  \n",
       "\n",
       "[433 rows x 9 columns]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "926e5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to process\n",
    "df.drop('directed_vs_generalized', inplace=True, axis=1)\n",
    "columns_to_process = ['violence', 'gender',\t'race',\t'national_origin',\t'disability',\t'religion',\t'sexual_orientation']\n",
    "\n",
    "# Process each cell in the specified columns\n",
    "for column in columns_to_process:\n",
    "    df[column] = df[column].apply(lambda x: 0 if x < 0.5 else 1 if x > 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "811e0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store column indices where 1s are located in each row\n",
    "columns_with_ones = []\n",
    "\n",
    "# Dictionary to map column names to corresponding indices (1 to 7)\n",
    "column_index_mapping = {\n",
    "    'violence': 1,\n",
    "    'gender': 2,\n",
    "    'race': 3,\n",
    "    'national_origin': 4,\n",
    "    'disability': 5,\n",
    "    'religion': 6,\n",
    "    'sexual_orientation': 7\n",
    "}\n",
    "\n",
    "# Iterate over rows and columns to find 1s and store column indices\n",
    "for _, row in df.iterrows():\n",
    "    indices = [column_index_mapping[column] for column in row.index if row[column] == 1]\n",
    "    columns_with_ones.append(','.join(map(str, indices)))\n",
    "\n",
    "# Create a new column in the DataFrame and add column indices with 1s\n",
    "df['Labels'] = columns_with_ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c43fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>violence</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>national_origin</th>\n",
       "      <th>disability</th>\n",
       "      <th>religion</th>\n",
       "      <th>sexual_orientation</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You should know women's sports are a joke</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You look like Sloth with deeper Down’s syndrome</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You look like Russian and speak like Indian. B...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women deserve to be abused, I guess.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1,2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women are made for making babies and cooking d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>All people are fucked up because of these quee...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Actually THIS WEIRD PEOPLE are HARMFUL! They n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>a homo? so you fuck your brother instead of yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>(((They))) will be chased out of every white c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  violence  gender  \\\n",
       "0            You should know women's sports are a joke         0       1   \n",
       "1      You look like Sloth with deeper Down’s syndrome         0       0   \n",
       "2    You look like Russian and speak like Indian. B...         1       0   \n",
       "3                 Women deserve to be abused, I guess.         1       1   \n",
       "4    Women are made for making babies and cooking d...         0       1   \n",
       "..                                                 ...       ...     ...   \n",
       "428  All people are fucked up because of these quee...         0       1   \n",
       "429  Actually THIS WEIRD PEOPLE are HARMFUL! They n...         0       0   \n",
       "430  ABU DA BI ABA DU BA BA HAMUD! See? I’m an arab...         0       0   \n",
       "431  a homo? so you fuck your brother instead of yo...         0       0   \n",
       "432  (((They))) will be chased out of every white c...         0       0   \n",
       "\n",
       "     race  national_origin  disability  religion  sexual_orientation Labels  \n",
       "0       0                0           0         0                   0      2  \n",
       "1       0                0           1         0                   0      5  \n",
       "2       0                1           0         0                   0    1,4  \n",
       "3       0                0           0         0                   0    1,2  \n",
       "4       0                0           0         0                   0      2  \n",
       "..    ...              ...         ...       ...                 ...    ...  \n",
       "428     0                0           0         0                   1    2,7  \n",
       "429     0                0           1         0                   0      5  \n",
       "430     0                0           0         1                   0      6  \n",
       "431     0                0           0         0                   1      7  \n",
       "432     1                0           0         0                   0      3  \n",
       "\n",
       "[433 rows x 9 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e1f4570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment               0\n",
       "violence              0\n",
       "gender                0\n",
       "race                  0\n",
       "national_origin       0\n",
       "disability            0\n",
       "religion              0\n",
       "sexual_orientation    0\n",
       "label                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369c4788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                               53\n",
       "race                                 53\n",
       "national_origin                      45\n",
       "sexual_orientation                   45\n",
       "religion                             45\n",
       "violence,religion                    32\n",
       "disability                           32\n",
       "violence,gender                      24\n",
       "violence,national_origin             22\n",
       "violence,sexual_orientation          22\n",
       "violence,disability                  20\n",
       "violence,race                        15\n",
       "                                      5\n",
       "gender,sexual_orientation             4\n",
       "violence                              3\n",
       "race,national_origin                  3\n",
       "violence,gender,religion              2\n",
       "race,sexual_orientation               2\n",
       "national_origin,disability            1\n",
       "gender,race,national_origin           1\n",
       "violence,national_origin,religion     1\n",
       "gender,race                           1\n",
       "national_origin,religion              1\n",
       "violence,gender,race                  1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input text\n",
    "df = \"Example hate speech text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cc684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text using BERT tokenizer\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True)\n",
    "input_ids = tf.constant(input_ids, dtype=tf.int32)\n",
    "input_ids = tf.expand_dims(input_ids, 0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af900c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass input through BERT model\n",
    "bert_output = bert_model(input_ids)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "input_layer = Input(shape=(bert_output.shape[1], bert_output.shape[2]), dtype=tf.float32)\n",
    "dense_layer = Dense(2, activation='softmax')(input_layer)  # 2 classes: hate speech or non-hate speech\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with cross-entropy loss and Adam optimizer\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example labels (0 for non-hate speech, 1 for hate speech)\n",
    "labels = tf.constant([1], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04334f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (in this example, using the same input as both input and labels)\n",
    "model.fit(bert_output, labels, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11682b8c",
   "metadata": {},
   "source": [
    "Import Libraries: Import necessary libraries including TensorFlow and Hugging Face's Transformers library.\n",
    "\n",
    "Initialize BERT Tokenizer and Model: Initialize the BERT tokenizer and model from Hugging Face's Transformers library.\n",
    "\n",
    "Tokenize Input Text: Tokenize the input text using the BERT tokenizer. The encode function converts the text to token IDs. The pad_to_max_length and truncation parameters ensure that the input has a fixed length suitable for BERT.\n",
    "\n",
    "Pass Input through BERT Model: Pass the tokenized input through the BERT model to obtain BERT embeddings for the input text.\n",
    "\n",
    "Build Neural Network Model: Build a neural network model. In this example, a dense layer with softmax activation is added on top of the BERT output. The output shape is set to 2, representing the two classes: hate speech and non-hate speech.\n",
    "\n",
    "Compile the Model: Compile the model with sparse categorical cross-entropy loss and the Adam optimizer.\n",
    "\n",
    "Example Labels: Define example labels (0 for non-hate speech, 1 for hate speech). In a real application, you would have a dataset with corresponding labels.\n",
    "\n",
    "Train the Model: Train the model. In this example, the same BERT output is used as both input and labels for simplicity. In a real scenario, you would have separate input and label data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c822832",
   "metadata": {},
   "source": [
    "Step 1: Importing Necessary Libraries\n",
    "First, we need to import the necessary libraries. We’ll be using the Huggingface Transformers library, which provides a straightforward interface for working with BERT and other transformer models.\n",
    "\n",
    "⚠ This code is experimental content and was generated by AI. Please refer to this code as experimental only since we cannot currently guarantee its validity\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "Step 2: Loading the Pretrained Tokenizer\n",
    "Next, we load the pretrained BERT tokenizer. For this example, we’ll use the ‘bert-base-uncased’ model, but you can replace this with any BERT model you prefer.\n",
    "\n",
    "⚠ This code is experimental content and was generated by AI. Please refer to this code as experimental only since we cannot currently guarantee its validity\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "Step 3: Adding New Tokens\n",
    "Now, we’re ready to add new tokens. The add_tokens method makes this easy. It takes a list of new tokens and returns the number of tokens actually added to the vocabulary.\n",
    "\n",
    "⚠ This code is experimental content and was generated by AI. Please refer to this code as experimental only since we cannot currently guarantee its validity\n",
    "\n",
    "new_tokens = ['newtoken1', 'newtoken2']\n",
    "num_added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print('Number of tokens added:', num_added_tokens)\n",
    "Step 4: Resizing Token Embeddings\n",
    "After adding new tokens, we need to resize the token embeddings of the BERT model to match the new size of the vocabulary. This is done using the resize_token_embeddings method.\n",
    "\n",
    "⚠ This code is experimental content and was generated by AI. Please refer to this code as experimental only since we cannot currently guarantee its validity\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "Step 5: Saving the Tokenizer\n",
    "Finally, we save the updated tokenizer for future use. The save_pretrained method saves both the tokenizer configuration and the vocabulary, which includes the new tokens.\n",
    "\n",
    "⚠ This code is experimental content and was generated by AI. Please refer to this code as experimental only since we cannot currently guarantee its validity\n",
    "\n",
    "tokenizer.save_pretrained('path_to_directory')\n",
    "Conclusion\n",
    "Adding new tokens to the Huggingface BERT tokenizer is a straightforward process that can greatly improve the efficiency of your NLP tasks. By following these steps, you can customize the tokenizer to better suit your specific needs.\n",
    "\n",
    "Remember, when adding new tokens, it’s important to also update your model’s token embeddings to match the new vocabulary size. And don’t forget to save your updated tokenizer for future use!\n",
    "\n",
    "BERT and its tokenizer are powerful tools in the NLP toolkit. With the ability to add new tokens, you can make these tools even more powerful and tailored to your specific use cases.\n",
    "\n",
    "Keywords: Huggingface, BERT, Tokenizer, Add New Tokens, NLP, Transformers, Python, Data Science, Machine Learning\n",
    "\n",
    "Meta Description: Learn how to add new tokens to the Huggingface BERT tokenizer in this comprehensive guide. Ideal for data scientists and machine learning practitioners working with NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722e86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
